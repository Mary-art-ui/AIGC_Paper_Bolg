### [A survey on rag meeting llms: Towards retrieval-augmented large language models](https://dl.acm.org/doi/abs/10.1145/3637528.3671470)

 [被引用次数：999](https://scholar.google.com/scholar?cites=4439195171371337250&as_sdt=80005&sciodt=0,11&hl=zh-CN)

Fan W, Ding Y, Ning L, et al. A survey on rag meeting llms: Towards retrieval-augmented large language models[C]//Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining. 2024: 6491-6501.

The Hong Kong Polytechnic University, HK SAR 

National University of Singapore, Singapore

这篇 survey 把“RAG + LLM”称为 **RA-LLMs**，并按**架构 / 训练策略 / 应用**三条主线梳理。

论文在引言里明确动机：LLM 仍有**领域知识缺口、幻觉、更新成本高**等限制，因此需要借助 RAG <u>引入外部可靠知识</u>来增强能力。

------

## 1）读懂这篇论文需要哪些基础知识（通俗版）

### A. 先理解“RAG/RA-LLM”最基本流水线

你需要知道：**检索（Retriever）**先从外部库找资料，**生成（Generator/LLM）**再用这些资料来回答。

```mermaid
flowchart LR
Q[用户问题 Query] --> R[Retriever 检索外部数据]
R --> C[Context/证据片段]
C --> G[LLM 生成]
G --> A[答案 Answer]
```

这篇 survey 强调：RAG 能带来“更及时、更可控、可解释、能减少幻觉”的潜力。

------

### B. 你要懂“检索”里面三件事：表示、索引、排序

1. **表示/Embedding**：把文本变成向量，便于相似度搜索。
2. **索引/ANN**：大规模近邻搜索（速度 vs 准确率权衡）。
3. **重排序/过滤**：先粗搜 top-k，再做更准的 rerank / 纠错。

（这些在很多 RAG 系统里是默认模块；这篇 survey 在 Table 1 里也用“检索器类型 + 是否训练检索器”等字段来对比经典模型。）

------

### C. 你要理解“怎么把检索结果喂给 LLM”：三种融合位置

这篇 survey 讲得很清楚：把检索到的信息融合进生成器有不同“插入点”：

- **输入层融合**：把证据拼到 prompt 里（最常见，也最容易落地）
- **输出/后融合**：生成后再校验、引用、纠错
- **中间层融合**：在 Transformer 的中间层做 cross-attention / memory 注入（更强，但需要改模型内部，不适合只能用 API 的闭源 LLM）

------

### D. 你要懂两个“行为控制”概念：何时检索 + 多频繁检索

论文专门一节讲 **Retrieval Necessity & Frequency**：

- **Necessity（是否需要检索）**：检索不是越多越好；无关证据会覆盖 LLM 已有正确知识，甚至让幻觉翻倍。
- **Frequency/Stride（检索频率）**：常见三档：一次检索 / 每 n 个 token 检索 / 每 token 检索。

------

## 2）用中文通俗解读这篇 survey（抓住关键内容）

### 2.1 论文到底在综述什么

它把“RAG + LLM”系统化为 **RA-LLMs**，从三条主线总结：

- **架构（architectures）**：检索器、生成器、增强/融合方式
- **训练策略（training strategies）**：怎么训检索器/生成器/两者协同（以及更贴近 LLM 的训练范式）
- **应用（applications）**：哪些任务最受益（需要最新、可靠、可追溯知识的场景）

------

### 2.2 一个“总览表”抓住方法谱系：Table 1

论文用 **Table 1** 汇总了一批高影响 RAG/RA-LLM 工作（如 REALM、RAG、FiD、RETRO、Atlas、FLARE、Self-RAG 等），并用字段对齐它们的关键设计：

- 时间、模型、引用
- 检索器类型（稀疏/稠密/搜索引擎等）
- 是否训练检索器（RetTrain）
- 检索增强细节（如 query rewriting / reranking / 频率策略等）
- 生成器与增强融合位置、评测任务类型
  （你读 Table 1 的方式：**先看“检索器 + 是否训练”，再看“增强/融合发生在哪一层”，最后看“评测任务”来理解它解决的痛点**。）

------

### 2.3 论文最想提醒你的“坑”：检索可能害你

这篇 survey 很强调一个常被忽视的点：**检索不是万能药**。
<u>如果检索到的段落不相关，LLM 反而更容易被“带偏”，甚至幻觉更严重。所以研究重点之一变成：**如何判断要不要检索、检索哪些、怎么用**。</u>

这也直接引出代表方法：

- **Self-RAG**：用特殊 token 让模型自我判断是否需要检索并控制检索行为。
- **FLARE**：当模型 logits 置信度低于阈值时触发检索（动态触发）。
- **SKR**：用“自我知识”作为参考，指导是否检索与如何检索。
- **SlimPLM**：用小代理模型生成“启发式答案”，用来判断缺什么知识并辅助 query rewriting。

------

### 2.4 把“关键方法谱系”串起来

```mermaid
mindmap
  root((RA-LLMs = RAG + LLM))
    Why(为什么需要)
      Hallucination(幻觉)
      DomainGap(领域/私有知识缺口)
      UpdateCost(更新模型成本高)
    Architecture(架构怎么搭)
      Retriever(检索器)
        Sparse(稀疏: BM25/搜索)
        Dense(稠密: DPR/BERT等)
        Hybrid(混合检索)
      Augmentation(怎么把证据用进LLM)
        InputFusion(输入层拼接Prompt)
        OutputFusion(输出后校验/纠错)
        IntermediateFusion(中间层Cross-Attn/Memory)
    Control(检索行为控制)
      Necessity(要不要检索)
        SelfRAG(Self-RAG: token控制)
        FLARE(低置信度触发)
        SKR(自我知识引导)
        SlimPLM(代理模型+启发式答案)
      Frequency(检索频率/stride)
        OneTime(一次检索)
        EveryN(每n-token检索)
        EveryToken(每token检索)
    EvidenceRisk(风险提醒)
      Irrelevant(无关证据会带偏)
      HallucinationUp(幻觉可能更高)
```

------

## 3）论文重要术语提取 + 通俗解释（带着你读 Table 1/相关段落）

> 我选的是这篇 survey 里“最承重”的术语（读 Table 1 和检索控制那节会反复遇到）。

### 3.1 RA-LLMs（Retrieval-Augmented Large Language Models）

把 LLM 的生成过程与外部检索结合：回答不只靠“模型记忆”，而是靠“**检索到的证据 + LLM 组织表达**”。

### 3.2 Retrieval Necessity（检索必要性）

核心问题：**这道题到底需不需要查资料**？
因为“查到不相关内容”会覆盖 LLM 原本正确知识，导致更糟答案。

### 3.3 Retrieval Frequency / Retrieval Stride（检索频率/步长）

决定检索发生的节奏：

- One-time（只查一次）
- Every-n-token（写一段再查）
- Every-token（几乎边写边查，最贵但可能更强）

### 3.4 Intermediate-layer Integration（中间层融合）

不是把证据拼到 prompt，而是把检索到的向量/块在 Transformer 中间层用 cross-attention / memory 注入（如 RETRO 的 Chunked Cross Attention）。优点是能更频繁、更高效处理大量 chunk；缺点是需要改模型内部，闭源 API 很难做。

### 3.5 Table 1（高影响模型对照表）

这张表是你快速“扫谱系”的入口：用统一字段对齐各 RAG/RA-LLM 的检索器、训练方式、增强方式与评测任务。

------


**Table 1 是整篇《RAG Meeting LLMs》survey 的“方法总索引”**，但原表是**线性文本**，不利于快速建立**方法谱系认知**。
下面把 **Table 1 系统性拆解 + 可视化**，做到：

1. **不丢信息（忠实于 Table 1 的字段）**
2. **一眼看清“方法差异点”**

------

# 一、先说清楚：Table 1 在“比较什么”？

Table 1 并不是随便列模型，而是围绕 **RA-LLMs 的关键设计维度** 对齐方法。
抽象后，字段本质是这 6 个问题：

| 维度         | Table 1 对应字段      | 核心问题                              |
| ------------ | --------------------- | ------------------------------------- |
| 时间         | Year                  | 技术演进脉络                          |
| 检索器       | Retriever             | 用什么方式找知识                      |
| 检索是否训练 | RetTrain              | 检索是“固定工具”还是“可学习模块”      |
| 检索增强     | Retrieval Enhancement | 是否做 query 改写 / rerank / 动态触发 |
| 融合位置     | Augmentation          | 知识在 LLM 哪一层起作用               |
| 任务         | Tasks                 | 解决什么问题                          |

👉 **这正是 Table 1 的“隐含方法论”**

------

# 二、Table 1 的“方法谱系总览图”

> **目标**：你一眼就知道
> **“某个方法在 RAG × LLM 设计空间的什么位置”**

```mermaid
mindmap
  root((Table 1: RA-LLMs 方法谱系))
    
    Early_RAG(早期 RAG)
      REALM
        Retriever:Dense
        RetTrain:Yes
        Fusion:Input
        Task:OpenQA
      RAG_Model
        Retriever:Dense
        RetTrain:Yes
        Fusion:Input
        Task:QA

    Fusion_Enhanced(生成融合增强)
      FiD
        Retriever:Dense
        RetTrain:Yes
        Fusion:EncoderFusion
        Task:MultiDocQA
      Atlas
        Retriever:Dense
        RetTrain:Yes
        Fusion:EncoderFusion
        Task:QA

    Intermediate_Layer(中间层增强)
      RETRO
        Retriever:ChunkSearch
        RetTrain:No
        Fusion:CrossAttention
        Task:LM_Pretrain
      KNN_LM
        Retriever:Vector
        RetTrain:No
        Fusion:Logits
        Task:LanguageModeling

    Retrieval_Control(检索行为控制)
      Self_RAG
        Trigger:TokenBased
        Control:Necessity
        Fusion:Input
      FLARE
        Trigger:ConfidenceBased
        Control:Dynamic
        Fusion:Input
      SKR
        Trigger:SelfKnowledge
        Control:Necessity

    Tool_or_Framework(系统/框架)
      LangChain
        Role:Pipeline
      LlamaIndex
        Role:Indexing_Framework
```



-  **RAG 不只是 prompt 拼接**

------

# 三、Table 1 的“检索设计维度”可视化

> **目标**：看清楚
> **哪些方法在“检索器”与“检索训练”上做文章**

```mermaid
flowchart TB
A[RA-LLMs] --> B{Retriever 类型}

B --> C[Sparse]
C --> C1[BM25 / Search Engine]

B --> D[Dense]
D --> D1[REALM]
D --> D2[RAG]
D --> D3[FiD]
D --> D4[Atlas]

B --> E[Chunk-based]
E --> E1[RETRO]

B --> F[Hybrid]
F --> F1[Dense + Rerank]

A --> G{Retriever 是否训练}
G --> G1[Yes]
G1 --> G1a[REALM]
G1 --> G1b[RAG]
G1 --> G1c[FiD]
G1 --> G1d[Atlas]

G --> G2[No]
G2 --> G2a[RETRO]
G2 --> G2b[KNN-LM]
```

📌 **重要洞察（Table 1 隐含但没直说）**

> **是否训练检索器，决定了这是“端到端模型”还是“系统型方法”**

------

# 四、Table 1 的“融合位置”可视化

> **目标**：一眼区分
> **Prompt 拼接 vs 模型内部改造**

```mermaid
flowchart LR
A[Retrieval Augmentation] --> B[Input-level]
A --> C[Intermediate-level]
A --> D[Output / Logits-level]

B --> B1[RAG]
B --> B2[FiD]
B --> B3[Self-RAG]
B --> B4[FLARE]

C --> C1[RETRO]
C --> C2[Atlas]

D --> D1[KNN-LM]
```

📌 **非常适合写论文时的一句话总结：**

> “Existing RA-LLMs differ mainly in *where* retrieved knowledge is injected into the generation pipeline.”

------

# 五、Table 1 的“演进时间线”

> **目标**：说明这是一个**逐步走向“控制与智能化”的过程**

```mermaid
timeline
    title Evolution of RA-LLMs (from Table 1)
    2020 : REALM, KNN-LM
    2021 : RAG, FiD
    2022 : RETRO, Atlas
    2023 : FLARE, Self-RAG, SKR
```

