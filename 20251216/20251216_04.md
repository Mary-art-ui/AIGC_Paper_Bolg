**“斯坦福小镇论文”**（2023，Stanford + Google）👇

> **Generative Agents: Interactive Simulacra of Human Behavior**
> *Joon Sung Park et al., Stanford University*
>
> https://github.com/joonspk-research/generative_agents

------

remember :记录信息。每个人可以看做一个记忆数据库，把看到的信息都记录进来。

reflect：反思。比如，我遇到一个人，该生成什么话，需要到记忆库检索和这个人有什么相关的，还要看我的plan，今天有没有什么和这个人相关的。

- 先**感知**（在周围的世界看到了什么，每一步都需要问大模型LLM），

- 再**思考**（看到的东西生成了什么记忆，对我来说意味着什么，用LLM总结），

- 再**反思**（思考了很多问题，但仅仅是结合看到的是，没有结合长期的考虑，反思就是结合这些思考做一个全局考虑，也是问LLM做进一步总结，生成新的特征）

- **制定计划**Plan

- **执行行为**

```mermaid
flowchart TB

%% 总体框架
A[斯坦福 AI 小镇]

A --> B[MAZE 环境 / 沙盒游戏]

%% 环境初始化信息（并列）
B --> C1[初始化坐标、地点信息]
B --> C2[物品、信息]

%% 四大并列模块
A --> D[感知]
A --> E[思考]
A --> F[规划]
A --> G[与环境交互并且做出行为 Act]

%% 感知模块内部结构
D --> D1[获取人可访问的半径]
D1 --> D2[在这个范围内感知事物，获取信息]
D --> D3[与当前记忆对比]
D3 -->|没有| D4[加入记忆中]

D --> D5[Embedding]
D5 --> D6[记忆中的都是向量]
D6 --> D7[为了检索]
D7 --> D8[打分]

%% 思考模块内部结构
E --> E1[低级思考]
E1 --> E1a[看见一个事物表面现象]
E1a --> E1b[事物的关联信息是什么样的]

E --> E2[高级思考]
E2 --> E2a[生成问题]
E2a --> E2b[我看到了一些事物，就产生了一些质疑]

E2 --> E2c[思考出来了啥东西]
E2c --> E2d[观察这些信息以及不理解的地方]
E2d --> E2e[让 LLM 生成]

E2 --> E3[把思考得到的东西放入记忆中]

%% 规划模块内部结构
F --> F1[生成接下来要做什么]
F --> F2[生成醒来的时间]

F --> F3[生成粗略的计划]
F3 --> F3a[根据时间、人物属性等基本信息]
F3a --> F3b[基于提示生成]
F3b --> F3c[今天要做几个事]
F3c --> F3d[在什么时间段]

F --> F4[生成详细的计划]
F4 --> F4a[每个小时做什么事 / 任务]
F4a --> F4b[再拆分每个小时要做啥]
F4b --> F4c[这五分钟具体做什么]

F --> F5[根据行为生成要交互所需的信息]
F5 --> F5a[地点]
F5 --> F5b[物品]
F5 --> F5c[事件]
F5 --> F5d[人]
F5 --> F5e[表情]


```



## 一、这篇论文在做什么？

> **用大语言模型 + 记忆检索机制，构建一个“会记忆、会规划、会社交”的虚拟人小镇。**

25 个 AI Agent 生活在一个虚拟小镇里：

- 会起床、工作、吃饭
- 会聊天、**<u>传播消息</u>**
- 会计划生日派对
- 行为**不是脚本写死的，而是“生成式”的**（Agent的每一句话都是LLM生成出来的）

👉 这不是游戏论文，而是 **Agent 架构 + 记忆检索（RAG-like）+ 行为生成** 的开创性工作。

------

## 二、为什么它叫“斯坦福小镇”？

论文中构建了一个 **Smallville（小镇）**：

- 地点：咖啡馆、房子、公园、商店
- 居民：25 个 AI Agent
- 时间：以“分钟”为粒度推进
- 每个 Agent 都由 **LLM 驱动**

你可以把它理解成：

> **一个“社会级别的、多智能体 RAG 系统”**

------

## 三、核心技术架构

论文的 Agent 不是直接「Prompt → LLM → 行为」，而是一个**三层认知循环**：

------

### 🧠 Agent 核心循环（论文原话）

```mermaid
flowchart LR
    %% External I/O
    Perceive[Perceive] --> MS[Memory Stream]
    RM[Retrieved Memories] --> Act[Act]

    %% Dashed box: Generative Agent Memory
    subgraph GAM[Generative Agent Memory]
        direction LR
        MS --> Ret[Retrieve] --> RM
    end

    %% Top loop (as in the figure): Retrieved Memories -> Plan -> Memory Stream
    RM --> Plan[Plan]
    Plan --> MS

    %% Bottom loop (as in the figure): Retrieved Memories -> Reflect -> Memory Stream
    RM --> Refl[Reflect]
    Refl --> MS

    %% Styling for dashed container
    style GAM stroke-dasharray: 6 6


```

感知模块：日志任务当中，随着时间推移不断更新，怎么更新保存重要的信息？/当前关注到的总结后的信息保存那些? /Agent每走一步要观察好多东西，哪一个影响最大？

- 时效性 （自动衰减）

- 重要性 （大模型重要性打分）

- 相关性（余弦相似度）[0,1]之间分数

思考模块：

先提一个问题，再抽取最相关的100个总结的信息，生成接下来的计划

### 1️⃣ Memory（记忆模块）【RAG 核心】

Agent 的所有经历都会被存成 **自然语言记忆**：

- “我今天早上 8 点在咖啡馆见到了 Alice”
- “Sam 提到他最近心情不好”
- “今天是我的生日”

📌 每条记忆都有：

- 内容（文本）
- 时间戳
- 重要性（importance score）
- 向量 embedding

👉 **这是一个典型的 RAG Memory Store**

------

### 2️⃣ Memory Retrieval（检索机制）

当 Agent 要思考 / 行动时，会从记忆库中**检索相关记忆**，基于三个维度打分：

| 维度       | 含义                                       |
| ---------- | ------------------------------------------ |
| Relevance  | 与当前问题的语义相关性（Embedding 相似度） |
| Recency    | 最近发生的事情权重大                       |
| Importance | 重要事件（如生日、冲突）权重大             |

📌 最终用加权公式选 Top-K 记忆
👉 **这一步 = RAG 的 Retriever**

------

### 3️⃣ Reflection（反思 / 抽象）

这是论文的一个**关键创新点**：

Agent 会定期对记忆进行“反思”，生成**高层总结**：

- 从多条低层记忆 → 抽象出一个结论
- 例如：
  - 多次聊天 → “Alice 和 Bob 最近关系变近了”
  - 多次负面事件 → “我最近压力很大”

📌 这些 **Reflection 本身也会被写回记忆库**

> 这相当于：
> **LLM 自动生成“长期知识 / 中间概念”**

------

### 4️⃣ Planning（规划）

Agent 会生成 **日程计划（Plan）**：

- “9:00–10:00 在家吃早饭”
- “10:00–12:00 去咖啡馆工作”
- “晚上给 Sam 办生日派对”

📌 Plan 也是文本
📌 会被不断更新（Re-plan）

------

### 5️⃣ Action（行为生成）

最终行为由 LLM 生成：

- 去哪里
- 做什么
- 和谁说话
- 说什么内容



## 四、为什么这篇论文这么重要？

### 1️⃣ 它不是 Prompt Engineering，而是 **Agent Architecture**

第一次系统性提出：

- **<u>记忆</u> ≠ 上下文窗口**
- 而是一个：
  - 可检索
  - 可反思
  - 可长期积累
    的<u>外部认知系统</u>

👉 这是 **Agent + RAG** 的经典范式

------

### 2️⃣ 它验证了：LLM 可以涌现“社会行为”

论文中出现了❤：

- <u>消息自然传播</u>
- <u>社交关系演化</u>
- <u>集体活动（派对）</u>
- <u>角色一致性（persona stability）</u>

而这些**都没有硬编码规则**（no pre-programmed）。

------

### 3️⃣ 它启发了后续几乎所有 Agent 框架

你现在看到的很多东西，源头❤都在这里：

- LangChain Memory
- AutoGPT / BabyAGI
- CrewAI / MetaGPT
- 社会仿真、多智能体博弈
- LLM + 记忆 + 规划

------

## 五、它和RAG 有什么关系？

如果你从 **RAG 视角**看这篇论文，会发现：

### ✅ 它本质是一个「行为级 RAG 系统」

| RAG 组件  | 斯坦福小镇对应                   |
| --------- | -------------------------------- |
| 文档库    | Agent 的长期记忆                 |
| Embedding | 记忆向量                         |
| Retriever | relevance + recency + importance |
| Generator | LLM 行为 / 对话 / 规划           |
| Rerank    | 加权打分                         |
| Chunk     | 单条自然语言记忆                 |

👉 区别只是：
**RAG 的“文档”变成了“人生经历”**

------

## 六、这篇论文的局限性

- 所有 Agent 共享同一个强 LLM（现实不可扩展）
- Memory 全是自然语言，**缺乏结构化约束**
- 没有严格评测指标（偏 qualitative）
- 行为“看起来像人”，但不可控、不可验证
- 成本极高（token 巨多）

------





